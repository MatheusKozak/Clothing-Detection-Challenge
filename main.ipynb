{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwufnHfvfSwv",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Report - Computer Vision\n",
        "\n",
        "\n",
        "## Bachelor's Degree in Computer Science / PUCPR\n",
        "\n",
        "**Prof. Rayson Laroca**\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`And the year here`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68-F6cD9VL-J",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Import the libs you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "itrIGnaaVQnH",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
            "    self._run_once()\n",
            "  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
            "    handle._run()\n",
            "  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"C:\\Users\\mathe\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_66796\\3485718805.py\", line 4, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py\", line 144, in <module>\n",
            "    from . import _api, _version, cbook, _docstring, rcsetup\n",
            "  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\rcsetup.py\", line 27, in <module>\n",
            "    from matplotlib.colors import Colormap, is_color_like\n",
            "  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\colors.py\", line 56, in <module>\n",
            "    from matplotlib import _api, _cm, cbook, scale\n",
            "  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\scale.py\", line 22, in <module>\n",
            "    from matplotlib.ticker import (\n",
            "  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\ticker.py\", line 138, in <module>\n",
            "    from matplotlib import transforms as mtransforms\n",
            "  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\transforms.py\", line 49, in <module>\n",
            "    from matplotlib._path import (\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "numpy.core.multiarray failed to import",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cv2_imshow\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Add others as needed (scikit-learn, TensorFlow/PyTorch, etc.)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py:144\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
            "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
            "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\colors.py:56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\scale.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[0;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[0;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\ticker.py:138\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[0;32m    140\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    142\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    143\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    151\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\transforms.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     53\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
          ]
        }
      ],
      "source": [
        "# Import all packages you need here\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Add others as needed (scikit-learn, TensorFlow/PyTorch, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdlz87dujOyK"
      },
      "source": [
        "# 1. Define the task and dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srRErSuejT2a"
      },
      "source": [
        "In this section:\n",
        "- Clearly define the chosen task;\n",
        "- Provide a description and link to the dataset;\n",
        "- Justify your choice and ensure dataset is archived for final submission;\n",
        "- If task ≠ image classification (detection, segmentation, OCR, etc.), explain how you will simplify Step 6 (traditional baseline)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPukX0EIzZ-r"
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed\n",
        "# DataSet: https://pucpredu-my.sharepoint.com/personal/rayson_santos_pucpr_br/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Frayson%5Fsantos%5Fpucpr%5Fbr%2FDocuments%2FPAR2025%2Ezip&parent=%2Fpersonal%2Frayson%5Fsantos%5Fpucpr%5Fbr%2FDocuments&ga=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTP1pmiPgDY_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# 2. Acquire the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIRXQ53th3am",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this section, **load the dataset** and carry out **initial processing** to ensure that data types are properly formatted for subsequent steps. The exact procedure may vary depending on how the data is acquired (e.g., directly from files or through well-known APIs) and also on how further steps will be conducted.\n",
        "\n",
        "**Important note**: if the dataset is excessively large or contains high-resolution images, **consider subsampling and/or resizing the images to a manageable size** to ensure computational feasibility — especially when using limited resources such as Google Colab.\n",
        "\n",
        "* Clearly justify the chosen subsampling strategy and confirm that the resulting data remains representative of the original dataset;\n",
        "* *This subsampling step may be conducted after \"Dataset analysis and visualization\" if you find it more adequate*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MI6JIUVU08Ug"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the repository as a ZIP file\n",
        "import os\n",
        "\n",
        "os.system(\"wget https://github.com/MatheusKozak/Clothing-Detection-Challenge/archive/refs/heads/main.zip -O Clothing-Detection-Challenge.zip\")\n",
        "\n",
        "# Unzip only the training_set and validation_set folders\n",
        "os.system(\"unzip -j Clothing-Detection-Challenge.zip 'Clothing-Detection-Challenge-main/training_set/*' -d training_set\")\n",
        "os.system(\"unzip -j Clothing-Detection-Challenge.zip 'Clothing-Detection-Challenge-main/validation_set/*' -d validation_set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrgPbMP8xjtN"
      },
      "outputs": [],
      "source": [
        "# To avoid 404 from kaggle i will put all the csv onto my drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Path para pasta de DADOS\n",
        "# To use ur own drive you must create the CV folder in the root then DATASET folder inside and put the CSV there with the names places in the cell under these\n",
        "caminho_dados = '/content/gdrive/MyDrive/CV/DATASET/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTKu4ryayE9l"
      },
      "outputs": [],
      "source": [
        "# Load from  Drive\n",
        "df_cloting = np.loadtxt(caminho_dados + 'df_cloting.csv', delimiter=',')\n",
        "df_fashion = np.loadtxt(caminho_dados + 'df_fashion.csv', delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y7WshR3nmZkL",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8R3Z5xgWyj1i"
      },
      "source": [
        "# 3. Dataset analysis and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ULcpwDnOyj1i"
      },
      "source": [
        "In this section, you should conduct a comprehensive analysis of the dataset, reporting on (but not limited to) the following aspects:\n",
        "\n",
        "- **Basic statistics:**\n",
        "  - Total number of images;\n",
        "  - Range and distribution of image resolutions;\n",
        "  - Number of classes.\n",
        "\n",
        "- **Class distribution:**\n",
        "  - Visualize with plots/histograms;\n",
        "  - Discuss potential **impacts of class imbalance** on model performance (e.g., bias toward majority classes).\n",
        "\n",
        "- **Examples:**\n",
        "  - Display representative samples from each class to illustrate dataset diversity and possible challenges (e.g., intra-class variation, inter-class similarity).\n",
        "\n",
        "- **Dataset splits:**\n",
        "  - If train/validation/test splits are already defined by the dataset authors, clearly describe them;\n",
        "  - If not, proceed to the next section (Step 4) where you will define and justify your splitting strategy.\n",
        "\n",
        "- **Data quality assessment:**\n",
        "  - Comment on factors such as blur, occlusions, illumination issues, noise, and mislabeled or ambiguous samples;\n",
        "  - Relate these observations to the **specific problem** being addressed (e.g., misclassification due to low resolution, object detection under motion blur, OCR legibility, etc.).\n",
        "\n",
        "- **Specialized tasks:**\n",
        "  - For **detection or segmentation** datasets, analyze spatial properties of the annotations, including object **sizes, locations, aspect ratios**, and potential edge cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5IFTWiTyj1i"
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxCO_VGGxPrv"
      },
      "source": [
        "# 4. Dataset splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvHLBw13yxJ8"
      },
      "source": [
        "In this section, you should split the dataset using the division provided by the dataset authors or the one widely adopted by the research community, if such a standard exists.\n",
        "\n",
        "If no predefined split is available, you must create your own training, validation, and test subsets. In this case, carefully select a splitting strategy that aligns with both the nature of your task and the characteristics of the dataset. **Be sure to explicitly explain and justify your reasoning**, making clear why your chosen approach is appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsNg9WltzYey"
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO8LN3MgzkFD"
      },
      "source": [
        "# 5. Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMPdNpHzzk-K"
      },
      "source": [
        "In this stage, you should apply preprocessing techniques tailored to your task. Start with fundamental steps such as resizing, padding, and format conversion to ensure consistency across the dataset.\n",
        "\n",
        "Depending on the characteristics of your data and the requirements of the task, you may also consider additional techniques, such as normalization or scaling of pixel values, color space transformations (e.g., RGB to grayscale or HSV), contrast and brightness adjustment, noise reduction or filtering, among others.\n",
        "\n",
        "In some cases, instead of tackling the full complexity of tasks such as object detection, segmentation, or OCR, *you may apply preprocessing steps to extract regions of interest and reframe the problem as a simpler classification task* (e.g., “target object vs. non-target object” in cropped patches), making subsequent baseline comparisons more feasible.\n",
        "\n",
        "When deciding which strategies to adopt, **take into account the nature of the task, the characteristics of the dataset, the computational limitations of the hardware, and the specific models or approaches you plan to use**. Explicitly explain and justify your choices, making clear why each preprocessing step is appropriate in your context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyP6deCRzlzs"
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asLMtcQh1ojq"
      },
      "source": [
        "# 6. Traditional approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y38kQRhg1s_B"
      },
      "source": [
        "In this section, you should implement a baseline solution using **traditional image processing and machine learning techniques**.\n",
        "\n",
        "Begin by extracting [handcrafted] features with classical methods that are suitable for your task, such as color histograms, texture descriptors (e.g., LBP), or gradient-based features (e.g., HOG).\n",
        "*   You are encouraged to search for and experiment with other  descriptors from the literature that may be more relevant to your dataset or problem.\n",
        "\n",
        "To improve robustness, consider combining multiple feature types through concatenation or feature fusion. Apply feature scaling or normalization where appropriate to ensure compatibility with the chosen classifiers.\n",
        "\n",
        "Next, train at least three different traditional classifiers (e.g., SVM, Random Forest, KNN) on the extracted features and evaluate their performance using relevant metrics for your task. Perform an **ablation study** by comparing the results obtained with different feature sets (individually and in combination) to better understand their contribution to performance.\n",
        "\n",
        "Naturally, provide a clear justification for each design choice, explaining why it is appropriate for your specific problem. Also, analyze and discuss the results in detail, highlighting patterns, strengths, and limitations, as well as insights gained from the ablation study."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWLoSQRg42tG"
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTNabJkGiZyU"
      },
      "source": [
        "# Checkpoint (Submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4k3Gv9Rie31"
      },
      "source": [
        "1. Save this report as a Jupyter Notebook (`.ipynb`);\n",
        "2. Export a copy of the report as a PDF file (`.pdf`), ensuring that all code cells are executed and their outputs are visible in the exported document;\n",
        "3. Compress all the files (the Jupyter Notebook, PDF) into a single ZIP archive (`EquipeXX.zip`);\n",
        "4. Upload the ZIP file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDlyU2X8yj1j"
      },
      "source": [
        "# 7. Data augmentation (**post-checkpoint!**)\n",
        "\n",
        "In this section, you should **design a data augmentation strategy** to increase both the diversity and robustness of your training data. Consider applying a variety of techniques, including but not limited to:\n",
        "\n",
        "- Color adjustments  \n",
        "- Rotation and scaling  \n",
        "- Horizontal/vertical flipping  \n",
        "- Blurring (e.g., Gaussian or motion blur)  \n",
        "- Additive noise  \n",
        "- Compression artifacts  \n",
        "- Geometric distortions (e.g., perspective or warping)  \n",
        "- Cropping  \n",
        "\n",
        "You are encouraged to explore specialized libraries such as **Albumentations**, which provide efficient and flexible augmentation pipelines.  \n",
        "\n",
        "Clearly specify whether augmentation will be applied **offline** (pre-generated before training) or **online** (generated dynamically during training). When dealing with class imbalance, adopt **targeted augmentation strategies** to increase the representation of minority classes.  \n",
        "\n",
        "Make sure to **justify** your design choices. Discuss how each selected augmentation technique relates to the **requirements of the task** and the **characteristics of the dataset**, highlighting why your decisions are appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0ONaiPJjB5H"
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YzsH1DrSeQD"
      },
      "source": [
        "# 8. Deep learning approach (**post-checkpoint!**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHjTSDxaSjnC"
      },
      "source": [
        "In this section, you should leverage **deep learning models** to address your task. You may explore one or both of the following strategies:\n",
        "\n",
        "1. **Training from scratch** – train a neural network architecture directly on your dataset;  \n",
        "2. **Transfer learning** – leverage pre-trained models, either by:\n",
        "   - Freezing the feature extractor and training only the classifier layers; or\n",
        "   - Performing fine-tuning, updating some or all pre-trained layers.\n",
        "\n",
        "Consider well-established model families such as EfficientNet, MobileNet, ResNet, VGG, and [efficient] Transformer-based architectures, choosing those most appropriate given your **task requirements** and **computational constraints**.  \n",
        "\n",
        "To optimize performance, experiment with key hyperparameters (e.g., optimizer, initial learning rate, batch size) and incorporate adaptive training techniques, such as:  \n",
        "- Learning rate schedulers (e.g., *ReduceLROnPlateau*);\n",
        "- Early stopping mechanisms to prevent overfitting.\n",
        "\n",
        "You must train **at least three distinct models** and evaluate their performance using **relevant quantitative metrics**. Compare the outcomes with the best-performing traditional approach (from Step 6). Additionally, include **qualitative analysis** (e.g., visualization of predictions, error inspection, or case studies) to better understand model behavior beyond numerical results.\n",
        "\n",
        "Whenever possible, conduct an **ablation study** to isolate the contribution of specific design choices (e.g., augmentation strategy, training from scratch vs. transfer learning, etc.) to justify your final pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIqI2G4jjDWD"
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua7B5nTmgbN7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Digest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRU17kEnftuy"
      },
      "source": [
        "In this section, you should **briefly summarize the main findings** of your project. Additionally, provide a **critical reflection** on your work and the effort invested throughout the module. Highlight the aspects you believe were handled successfully, as well as those that, in hindsight, you would approach differently.\n",
        "\n",
        "As part of this reflection, consider whether the outcomes and lessons learned from this project contribute meaningfully to your overall **academic development** and to your **readiness for professional or postgraduate pursuits after graduation**.\n",
        "\n",
        "While this digest **must contain at least 2,500 characters**, it is intended solely for self-reflection purposes and **will not be considered in the formal evaluation of your work**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQWFN0TEPUzp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "```\n",
        "Add your text here.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CaFQEil1F6Q",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Final Steps (Submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2R5Kily1H7f",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "1. Save the complete report (including all steps from Step 1 onward) as a Jupyter Notebook (`.ipynb`);\n",
        "2. Export a copy of the report as a PDF file (`.pdf`), ensuring that all code cells are executed and their outputs are visible in the exported document;\n",
        "3. Compress all the files (the Jupyter Notebook, PDF) into a single ZIP archive (`EquipeXX.zip`);\n",
        "4. Upload the ZIP file to Canvas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
