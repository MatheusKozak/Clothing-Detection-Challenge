{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwufnHfvfSwv",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Report - Computer Vision\n",
        "\n",
        "\n",
        "## Bachelor's Degree in Computer Science / PUCPR\n",
        "\n",
        "**Prof. Rayson Laroca**\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`Place your name here` - `and your email here`\n",
        "\n",
        "`And the year here`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68-F6cD9VL-J",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Import the libs you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itrIGnaaVQnH",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Import all packages you need here\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Add others as needed (scikit-learn, TensorFlow/PyTorch, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Define the task and dataset"
      ],
      "metadata": {
        "id": "gdlz87dujOyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section:\n",
        "- Clearly define the chosen task;\n",
        "- Provide a description and link to the dataset;\n",
        "- Justify your choice and ensure dataset is archived for final submission;\n",
        "- If task ≠ image classification (detection, segmentation, OCR, etc.), explain how you will simplify Step 6 (traditional baseline)."
      ],
      "metadata": {
        "id": "srRErSuejT2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use as many code and text cells as needed\n",
        "# First DataSet: https://pucpredu-my.sharepoint.com/personal/rayson_santos_pucpr_br/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Frayson%5Fsantos%5Fpucpr%5Fbr%2FDocuments%2FPAR2025%2Ezip&parent=%2Fpersonal%2Frayson%5Fsantos%5Fpucpr%5Fbr%2FDocuments&ga=1\n",
        "# Second DatSet: https://www.kaggle.com/datasets/agrigorev/clothing-dataset-full\n",
        "# Third DataSet: https://www.kaggle.com/datasets/thusharanair/deepfashion2-original-with-dataframes\n",
        "\n",
        "#Assume that we can use one of those to validate the model, as a cross validation"
      ],
      "metadata": {
        "id": "VPukX0EIzZ-r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTP1pmiPgDY_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# 2. Acquire the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIRXQ53th3am",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this section, **load the dataset** and carry out **initial processing** to ensure that data types are properly formatted for subsequent steps. The exact procedure may vary depending on how the data is acquired (e.g., directly from files or through well-known APIs) and also on how further steps will be conducted.\n",
        "\n",
        "**Important note**: if the dataset is excessively large or contains high-resolution images, **consider subsampling and/or resizing the images to a manageable size** to ensure computational feasibility — especially when using limited resources such as Google Colab.\n",
        "\n",
        "* Clearly justify the chosen subsampling strategy and confirm that the resulting data remains representative of the original dataset;\n",
        "* *This subsampling step may be conducted after \"Dataset analysis and visualization\" if you find it more adequate*."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have a CSV with the ID and the Label and in another folder all the imagens\n",
        "# So this class is to smash all together\n",
        "\n",
        "class DataSet:\n",
        "    def __init__(self, ids, imgs , labels):\n",
        "        self.ids = ids\n",
        "        self.imgs = imgs\n",
        "        self.labels = labels\n",
        "\n",
        "    def find_by_id(self, id):\n",
        "        index = np.where(self.ids == id)[0]\n",
        "        if len(index) > 0:\n",
        "            index = index[0]\n",
        "            return {\n",
        "                'id': self.ids[index],\n",
        "                'image': self.imgs[index],\n",
        "                'label': self.labels[index]\n",
        "            }\n",
        "        else:\n",
        "            return None"
      ],
      "metadata": {
        "id": "MI6JIUVU08Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To avoid 404 from kaggle i will put all the csv onto my drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Path para pasta de DADOS\n",
        "# To use ur own drive you must create the CV folder in the root then DATASET folder inside and put the CSV there with the names places in the cell under these\n",
        "caminho_dados = '/content/gdrive/MyDrive/CV/DATASET/'\n"
      ],
      "metadata": {
        "id": "VrgPbMP8xjtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load from  Drive\n",
        "df_cloting = np.loadtxt(caminho_dados + 'df_cloting.csv', delimiter=',')\n",
        "df_fashion = np.loadtxt(caminho_dados + 'df_fashion.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "YTKu4ryayE9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y7WshR3nmZkL",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Dataset analysis and visualization"
      ],
      "metadata": {
        "collapsed": false,
        "id": "8R3Z5xgWyj1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you should conduct a comprehensive analysis of the dataset, reporting on (but not limited to) the following aspects:\n",
        "\n",
        "- **Basic statistics:**\n",
        "  - Total number of images;\n",
        "  - Range and distribution of image resolutions;\n",
        "  - Number of classes.\n",
        "\n",
        "- **Class distribution:**\n",
        "  - Visualize with plots/histograms;\n",
        "  - Discuss potential **impacts of class imbalance** on model performance (e.g., bias toward majority classes).\n",
        "\n",
        "- **Examples:**\n",
        "  - Display representative samples from each class to illustrate dataset diversity and possible challenges (e.g., intra-class variation, inter-class similarity).\n",
        "\n",
        "- **Dataset splits:**\n",
        "  - If train/validation/test splits are already defined by the dataset authors, clearly describe them;\n",
        "  - If not, proceed to the next section (Step 4) where you will define and justify your splitting strategy.\n",
        "\n",
        "- **Data quality assessment:**\n",
        "  - Comment on factors such as blur, occlusions, illumination issues, noise, and mislabeled or ambiguous samples;\n",
        "  - Relate these observations to the **specific problem** being addressed (e.g., misclassification due to low resolution, object detection under motion blur, OCR legibility, etc.).\n",
        "\n",
        "- **Specialized tasks:**\n",
        "  - For **detection or segmentation** datasets, analyze spatial properties of the annotations, including object **sizes, locations, aspect ratios**, and potential edge cases."
      ],
      "metadata": {
        "collapsed": false,
        "id": "ULcpwDnOyj1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# use as many code and text cells as needed"
      ],
      "metadata": {
        "id": "y5IFTWiTyj1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Dataset splitting"
      ],
      "metadata": {
        "id": "pxCO_VGGxPrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you should split the dataset using the division provided by the dataset authors or the one widely adopted by the research community, if such a standard exists.\n",
        "\n",
        "If no predefined split is available, you must create your own training, validation, and test subsets. In this case, carefully select a splitting strategy that aligns with both the nature of your task and the characteristics of the dataset. **Be sure to explicitly explain and justify your reasoning**, making clear why your chosen approach is appropriate."
      ],
      "metadata": {
        "id": "rvHLBw13yxJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use as many code and text cells as needed"
      ],
      "metadata": {
        "id": "GsNg9WltzYey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Image Preprocessing"
      ],
      "metadata": {
        "id": "KO8LN3MgzkFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this stage, you should apply preprocessing techniques tailored to your task. Start with fundamental steps such as resizing, padding, and format conversion to ensure consistency across the dataset.\n",
        "\n",
        "Depending on the characteristics of your data and the requirements of the task, you may also consider additional techniques, such as normalization or scaling of pixel values, color space transformations (e.g., RGB to grayscale or HSV), contrast and brightness adjustment, noise reduction or filtering, among others.\n",
        "\n",
        "In some cases, instead of tackling the full complexity of tasks such as object detection, segmentation, or OCR, *you may apply preprocessing steps to extract regions of interest and reframe the problem as a simpler classification task* (e.g., “target object vs. non-target object” in cropped patches), making subsequent baseline comparisons more feasible.\n",
        "\n",
        "When deciding which strategies to adopt, **take into account the nature of the task, the characteristics of the dataset, the computational limitations of the hardware, and the specific models or approaches you plan to use**. Explicitly explain and justify your choices, making clear why each preprocessing step is appropriate in your context."
      ],
      "metadata": {
        "id": "YMPdNpHzzk-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use as many code and text cells as needed"
      ],
      "metadata": {
        "id": "DyP6deCRzlzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Traditional approach"
      ],
      "metadata": {
        "id": "asLMtcQh1ojq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you should implement a baseline solution using **traditional image processing and machine learning techniques**.\n",
        "\n",
        "Begin by extracting [handcrafted] features with classical methods that are suitable for your task, such as color histograms, texture descriptors (e.g., LBP), or gradient-based features (e.g., HOG).\n",
        "*   You are encouraged to search for and experiment with other  descriptors from the literature that may be more relevant to your dataset or problem.\n",
        "\n",
        "To improve robustness, consider combining multiple feature types through concatenation or feature fusion. Apply feature scaling or normalization where appropriate to ensure compatibility with the chosen classifiers.\n",
        "\n",
        "Next, train at least three different traditional classifiers (e.g., SVM, Random Forest, KNN) on the extracted features and evaluate their performance using relevant metrics for your task. Perform an **ablation study** by comparing the results obtained with different feature sets (individually and in combination) to better understand their contribution to performance.\n",
        "\n",
        "Naturally, provide a clear justification for each design choice, explaining why it is appropriate for your specific problem. Also, analyze and discuss the results in detail, highlighting patterns, strengths, and limitations, as well as insights gained from the ablation study."
      ],
      "metadata": {
        "id": "y38kQRhg1s_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use as many code and text cells as needed"
      ],
      "metadata": {
        "id": "WWLoSQRg42tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint (Submission)"
      ],
      "metadata": {
        "id": "rTNabJkGiZyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Save this report as a Jupyter Notebook (`.ipynb`);\n",
        "2. Export a copy of the report as a PDF file (`.pdf`), ensuring that all code cells are executed and their outputs are visible in the exported document;\n",
        "3. Compress all the files (the Jupyter Notebook, PDF) into a single ZIP archive (`EquipeXX.zip`);\n",
        "4. Upload the ZIP file to Canvas."
      ],
      "metadata": {
        "id": "U4k3Gv9Rie31"
      }
    },
    {
      "metadata": {
        "id": "BDlyU2X8yj1j"
      },
      "cell_type": "markdown",
      "source": [
        "# 7. Data augmentation (**post-checkpoint!**)\n",
        "\n",
        "In this section, you should **design a data augmentation strategy** to increase both the diversity and robustness of your training data. Consider applying a variety of techniques, including but not limited to:\n",
        "\n",
        "- Color adjustments  \n",
        "- Rotation and scaling  \n",
        "- Horizontal/vertical flipping  \n",
        "- Blurring (e.g., Gaussian or motion blur)  \n",
        "- Additive noise  \n",
        "- Compression artifacts  \n",
        "- Geometric distortions (e.g., perspective or warping)  \n",
        "- Cropping  \n",
        "\n",
        "You are encouraged to explore specialized libraries such as **Albumentations**, which provide efficient and flexible augmentation pipelines.  \n",
        "\n",
        "Clearly specify whether augmentation will be applied **offline** (pre-generated before training) or **online** (generated dynamically during training). When dealing with class imbalance, adopt **targeted augmentation strategies** to increase the representation of minority classes.  \n",
        "\n",
        "Make sure to **justify** your design choices. Discuss how each selected augmentation technique relates to the **requirements of the task** and the **characteristics of the dataset**, highlighting why your decisions are appropriate."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use as many code and text cells as needed"
      ],
      "metadata": {
        "id": "H0ONaiPJjB5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Deep learning approach (**post-checkpoint!**)"
      ],
      "metadata": {
        "id": "4YzsH1DrSeQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you should leverage **deep learning models** to address your task. You may explore one or both of the following strategies:\n",
        "\n",
        "1. **Training from scratch** – train a neural network architecture directly on your dataset;  \n",
        "2. **Transfer learning** – leverage pre-trained models, either by:\n",
        "   - Freezing the feature extractor and training only the classifier layers; or\n",
        "   - Performing fine-tuning, updating some or all pre-trained layers.\n",
        "\n",
        "Consider well-established model families such as EfficientNet, MobileNet, ResNet, VGG, and [efficient] Transformer-based architectures, choosing those most appropriate given your **task requirements** and **computational constraints**.  \n",
        "\n",
        "To optimize performance, experiment with key hyperparameters (e.g., optimizer, initial learning rate, batch size) and incorporate adaptive training techniques, such as:  \n",
        "- Learning rate schedulers (e.g., *ReduceLROnPlateau*);\n",
        "- Early stopping mechanisms to prevent overfitting.\n",
        "\n",
        "You must train **at least three distinct models** and evaluate their performance using **relevant quantitative metrics**. Compare the outcomes with the best-performing traditional approach (from Step 6). Additionally, include **qualitative analysis** (e.g., visualization of predictions, error inspection, or case studies) to better understand model behavior beyond numerical results.\n",
        "\n",
        "Whenever possible, conduct an **ablation study** to isolate the contribution of specific design choices (e.g., augmentation strategy, training from scratch vs. transfer learning, etc.) to justify your final pipeline."
      ],
      "metadata": {
        "id": "uHjTSDxaSjnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use as many code and text cells as needed"
      ],
      "metadata": {
        "id": "wIqI2G4jjDWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua7B5nTmgbN7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Digest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you should **briefly summarize the main findings** of your project. Additionally, provide a **critical reflection** on your work and the effort invested throughout the module. Highlight the aspects you believe were handled successfully, as well as those that, in hindsight, you would approach differently.\n",
        "\n",
        "As part of this reflection, consider whether the outcomes and lessons learned from this project contribute meaningfully to your overall **academic development** and to your **readiness for professional or postgraduate pursuits after graduation**.\n",
        "\n",
        "While this digest **must contain at least 2,500 characters**, it is intended solely for self-reflection purposes and **will not be considered in the formal evaluation of your work**."
      ],
      "metadata": {
        "id": "FRU17kEnftuy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQWFN0TEPUzp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "```\n",
        "Add your text here.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CaFQEil1F6Q",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Final Steps (Submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2R5Kily1H7f",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "1. Save the complete report (including all steps from Step 1 onward) as a Jupyter Notebook (`.ipynb`);\n",
        "2. Export a copy of the report as a PDF file (`.pdf`), ensuring that all code cells are executed and their outputs are visible in the exported document;\n",
        "3. Compress all the files (the Jupyter Notebook, PDF) into a single ZIP archive (`EquipeXX.zip`);\n",
        "4. Upload the ZIP file to Canvas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}